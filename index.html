<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.0.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">
  <meta name="google-site-verification" content="-jCet6qPD7nML4-W6IY2HZwfSblJyWta_Jx7_AGHgIk">
  <meta name="baidu-site-verification" content="f349dc627a72ee616d62ed513574025d">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/themes/orange/pace-theme-center-circle.css">
  <script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js" integrity="sha256-gqd7YTjg/BtfqWSwsJOvndl0Bxc8gFImLEkXQT8+qj0=" crossorigin="anonymous"></script>

<script class="next-config" data-name="main" type="application/json">{"hostname":"weirdozz.github.io","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.9.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":false,"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}}</script><script src="/js/config.js"></script>
<meta name="description" content="怕什么真理无穷，进一步有进一步的欢喜">
<meta property="og:type" content="website">
<meta property="og:title" content="Weirdo">
<meta property="og:url" content="https://weirdozz.github.io/index.html">
<meta property="og:site_name" content="Weirdo">
<meta property="og:description" content="怕什么真理无穷，进一步有进一步的欢喜">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="Weirdo">
<meta property="article:tag" content="WeirdoBlog">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://weirdozz.github.io/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"zh-CN","comments":"","permalink":"","path":"index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Weirdo</title>
  




  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">Weirdo</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li>
        <li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li>
        <li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li>
        <li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
        <li class="menu-item menu-item-movies"><a href="/movies/" rel="section"><i class="fa fa-film fa-fw"></i>电影</a></li>
        <li class="menu-item menu-item-books"><a href="/books/" rel="section"><i class="fa fa-book fa-fw"></i>书籍</a></li>
  </ul>
</nav>




</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Weirdo"
      src="/images/weirdo.jpg">
  <p class="site-author-name" itemprop="name">Weirdo</p>
  <div class="site-description" itemprop="description">怕什么真理无穷，进一步有进一步的欢喜</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">21</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">10</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">16</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://weirdozz.github.io/2022/03/25/Human-level-concept-learning-through-probabilistic-program-induction/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/weirdo.jpg">
      <meta itemprop="name" content="Weirdo">
      <meta itemprop="description" content="怕什么真理无穷，进一步有进一步的欢喜">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Weirdo">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/03/25/Human-level-concept-learning-through-probabilistic-program-induction/" class="post-title-link" itemprop="url">Human-level concept learning through probabilistic program induction</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2022-03-25 13:18:32 / 修改时间：15:07:57" itemprop="dateCreated datePublished" datetime="2022-03-25T13:18:32+08:00">2022-03-25</time>
    </span>

  
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Disqus：</span>
    
    <a title="disqus" href="/2022/03/25/Human-level-concept-learning-through-probabilistic-program-induction/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2022/03/25/Human-level-concept-learning-through-probabilistic-program-induction/" itemprop="commentCount"></span>
    </a>
  </span>
  
  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="摘要">摘要</h2>
<p>人类学习一个新的概念通常只需要从极其少量的实例中就能进行概括归纳，但是机器学习算法则需要大量的样本才能达到相同的精度。</p>
<p>此外人类学习的比机器学习的具有更丰富更广泛的拓展，人类可以基于现有的类创建新的抽象对象类别，而机器分类器不会执行这些额外的功能，即便要执行，也会需要一个专门的新算法。</p>
<p><img src="https://i.imgur.com/eytNsjy.png" alt="picture 1"></p>
<p>对于这些新算法面临的挑战在于：</p>
<ul>
<li>人们如何从少量例子学习新的概念？</li>
<li>人们如何学习抽象、丰富、灵活的表示？</li>
<li>如何从稀疏的数据中成功学习，同时产生如此丰富的表示？</li>
</ul>
<h2 id="引言">引言</h2>
<p>本文介绍了一个贝叶斯程序学习（BPL）框架，能够仅从一个示例中学习一大类视觉概念，并且以人类几乎无法区分的方式进行泛化。</p>
<p>概念用简单的概率程序进行表示。这样，丰富的概念就可以从更简单的概念组合构成。</p>
<p>学习则是通过构建最能解释贝叶斯标准下的观察结果的程序来进行的，而模型去学会学习是通过开发层次鲜艳，允许之前对相关概念的经验来简化新概念的学习。简单来说就是，该框架可以通过重用现有的程序片段来构建新的程序，捕捉因果和组合。</p>
<h2 id="贝叶斯程序学习">贝叶斯程序学习</h2>
<p>BPL通过学习简单的随机程序来表示概念，通过下图中的part(A,iii),subpart(A,ii)和空间关系(A,iv)来构建他们。</p>
<p>BPL定义了一个生成模型，可以通过新的方式组合part和subpart来对新类型的概念进行采样。</p>
<p>每个新的类型也被表示为一个生成模型，并且产生了新概念的实例(A,v)，这样BPL就成为了生成模型的生成模型。最后用原始数据的格式对生成的实例进行展示。</p>
<p>在随后的评估任务中，既不使用产生的数据，也不使用该集合中的任何字母，这些任务只提供新字符的原始图像。</p>
<p>手写字符的类型$\Psi$是part、subpart和空间关系的抽象模式。字符的part$S_i$是按下笔开始到提起笔结束的笔画，而subpart则是由短暂的停笔所分隔的更原始的动作。</p>
<p>首先是type层面的内容：</p>
<p>为了构建一个新的字符类型，首先需要采样k个parts并且对于每一个part i=1…k取$n_i$个subparts。一个part $S_i$的模板是通过从一组离散原始的动作中采样来的subpart构成的,这些原始动作是从背景集中学来的，这样下一个动作的概率会依赖上一个动作。然后通过对每一个subparts之间的控制点和尺度参数进行取样将parts表示成参数化的曲线。最后parts之间的位置关系由$R_i$决定。</p>
<p>到这一步的时候就已经得到字符的解析结构了，接下来就要进入到token层面的过程了：</p>
<p>首先引入适当的噪声来生成笔画曲线S(m)，然后从背景集中得到笔画空间位置关系结合上一笔，取样就能得到part的开始位置。然后进行放射变换A(m)并加入适当噪声。最后通过随机补偿函数得到二值图像，画出轨迹。</p>
<p><img src="https://i.imgur.com/RqMOnN2.png" alt="picture 2"></p>
<p>字符的token$\theta^{(m)}$是通过执行part和关系并且对墨迹流向建模产生的。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://weirdozz.github.io/2022/03/23/%E7%AE%80%E5%8D%95%E6%98%93%E6%87%82%E5%9C%B0%E7%90%86%E8%A7%A3LSTM/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/weirdo.jpg">
      <meta itemprop="name" content="Weirdo">
      <meta itemprop="description" content="怕什么真理无穷，进一步有进一步的欢喜">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Weirdo">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/03/23/%E7%AE%80%E5%8D%95%E6%98%93%E6%87%82%E5%9C%B0%E7%90%86%E8%A7%A3LSTM/" class="post-title-link" itemprop="url">简单易懂地理解LSTM</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-23 17:45:40" itemprop="dateCreated datePublished" datetime="2022-03-23T17:45:40+08:00">2022-03-23</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2022-03-24 12:35:10" itemprop="dateModified" datetime="2022-03-24T12:35:10+08:00">2022-03-24</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/DeepLearning/" itemprop="url" rel="index"><span itemprop="name">DeepLearning</span></a>
        </span>
    </span>

  
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Disqus：</span>
    
    <a title="disqus" href="/2022/03/23/%E7%AE%80%E5%8D%95%E6%98%93%E6%87%82%E5%9C%B0%E7%90%86%E8%A7%A3LSTM/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2022/03/23/简单易懂地理解LSTM/" itemprop="commentCount"></span>
    </a>
  </span>
  
  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="lstm结构">lstm结构</h2>
<p><img src="https://i.imgur.com/YTwGUXM.png" alt="picture 5"></p>
<p>相较于传统的RNN只有一个状态值传递，lstm传递了两个状态，其中c(cell state)变化地比较慢，而h(hidden state)则在不同节点下往往有很大的区别</p>
<h2 id="深入lstm结构">深入lstm结构</h2>
<p>首先用lstm当前的输入和上层传递过来的$h^{t-1}$拼接训练得到四个状态</p>
<p><img src="https://i.imgur.com/T581bEb.png" alt="picture 6"></p>
<p>接下来介绍这四个状态在LSTM内部的使用</p>
<p><img src="https://i.imgur.com/Vsx3k7N.png" alt="picture 7"></p>
<p>lstm主要有三个阶段：</p>
<ol>
<li>忘记阶段。对上个节点传过来的输入做选择性忘记。即通过$z^f$作为忘记门控，来控制上一个状态的$c^{t-1}$</li>
<li>选择记忆。将这个阶段的输入进行有选择性地记忆。选择门控信号由$z^i$控制。到这里将这两步的结果进行相加就可以得到传递给下一时间步的$c^t$</li>
<li>输出阶段。通过$z^0$控制当前状态的输出应该是什么。</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://weirdozz.github.io/2022/03/23/%E7%AE%80%E5%8D%95%E6%98%93%E6%87%82%E5%9C%B0%E7%90%86%E8%A7%A3GRU/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/weirdo.jpg">
      <meta itemprop="name" content="Weirdo">
      <meta itemprop="description" content="怕什么真理无穷，进一步有进一步的欢喜">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Weirdo">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/03/23/%E7%AE%80%E5%8D%95%E6%98%93%E6%87%82%E5%9C%B0%E7%90%86%E8%A7%A3GRU/" class="post-title-link" itemprop="url">简单易懂地理解GRU</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-23 16:35:02" itemprop="dateCreated datePublished" datetime="2022-03-23T16:35:02+08:00">2022-03-23</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2022-03-24 12:35:10" itemprop="dateModified" datetime="2022-03-24T12:35:10+08:00">2022-03-24</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/DeepLearning/" itemprop="url" rel="index"><span itemprop="name">DeepLearning</span></a>
        </span>
    </span>

  
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Disqus：</span>
    
    <a title="disqus" href="/2022/03/23/%E7%AE%80%E5%8D%95%E6%98%93%E6%87%82%E5%9C%B0%E7%90%86%E8%A7%A3GRU/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2022/03/23/简单易懂地理解GRU/" itemprop="commentCount"></span>
    </a>
  </span>
  
  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="GRU的输入输出结构">GRU的输入输出结构</h2>
<p>它的输入是当前时刻的输入$x^t$和上一时刻传递过来的隐状态$h^{t-1}$，通过对这两个数据的结合进行操作，会得到当前节点的输出$y^t$和传递给下一个节点的隐状态$h^t$</p>
<p><img src="https://i.imgur.com/Lb0dS15.png" alt="picture 1"></p>
<h2 id="GRU的内部结构">GRU的内部结构</h2>
<p>其中有两个门控状态。r控制重置门，z控制更新门，具体计算过程如下图所示</p>
<p><img src="https://i.imgur.com/25vkcJn.png" alt="picture 2"></p>
<p>得到门控信号之后，首先使用重置门的数据来得到重置后的$h^{t-1’}=h^{t-1}\odot r$，再将$h^{t-1’}$与输入进行拼接得到下图</p>
<p><img src="https://i.imgur.com/ulcM3RC.png" alt="picture 3"></p>
<p>这里的$h’$主要包含了当前输入的$x^t$数据，相当于记忆了当前时刻的状态。</p>
<p>最后一步是进行更新，更新表达式如下</p>
<p><img src="https://i.imgur.com/U9JlMYQ.png" alt="picture 4"></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://weirdozz.github.io/2022/03/20/Semi-Supervised-Classification-With-Graph-Convolutional-Networks/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/weirdo.jpg">
      <meta itemprop="name" content="Weirdo">
      <meta itemprop="description" content="怕什么真理无穷，进一步有进一步的欢喜">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Weirdo">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/03/20/Semi-Supervised-Classification-With-Graph-Convolutional-Networks/" class="post-title-link" itemprop="url">Semi-Supervised Classification With Graph Convolutional Networks</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-20 19:04:27" itemprop="dateCreated datePublished" datetime="2022-03-20T19:04:27+08:00">2022-03-20</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2022-03-24 12:35:10" itemprop="dateModified" datetime="2022-03-24T12:35:10+08:00">2022-03-24</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" itemprop="url" rel="index"><span itemprop="name">图神经网络</span></a>
        </span>
    </span>

  
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Disqus：</span>
    
    <a title="disqus" href="/2022/03/20/Semi-Supervised-Classification-With-Graph-Convolutional-Networks/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2022/03/20/Semi-Supervised-Classification-With-Graph-Convolutional-Networks/" itemprop="commentCount"></span>
    </a>
  </span>
  
  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="引言-2">引言</h2>
<p>文章主要围绕对仅有少部分节点存在标签的图节点进行分类任务。这个问题可以被认为是基于图的半监督学习，通过某种基于图的正则化来平滑图上的标签信息，比如在损失函数中增加一个图拉普拉斯正则化。</p>
<p><img src="https://i.imgur.com/q31vbLg.png" alt="picture 44"></p>
<p>其中$L_0$是有标签部分的损失项，函数f是一个可微函数，$\lambda$是一个权重参数，X是节点的特征向量矩阵，$\delta =D-A$是一个未归一化的图拉普拉斯矩阵，A是邻接矩阵，D是度矩阵。**公式一是居于相邻节点可能会共享相同标签的假设下的。**然而这种假设会限制模型的能力，因为图的边不一定是表示节点相似性的，可能是不相似性！</p>
<p>文中作者直接使用神经网络模型$f(X,A)$来编码整个图结构并且在所有有监督的节点上获取loss进行训练，因此就避免了损失函数中基于图的显示正则化。在图的邻接矩阵上调节函数f会使得模型能够从监督损失中分配梯度信息并且使它既能学习有标签也能学习无标签节点的表示。</p>
<p>主要贡献有两点：</p>
<ol>
<li>提出了一个简单有效的神经网络模型的传播规则，能够直接用于图并且展示了它是如何从一阶近似的谱图卷积得来的。</li>
<li>解释了这种模型是如何应用于快速且可扩展的节点半监督分类任务中的。</li>
</ol>
<h2 id="图的快速近似卷积">图的快速近似卷积</h2>
<p>下面是本文中GCN的逐层传播规则：</p>
<p><img src="https://i.imgur.com/dEpVbPD.png" alt="picture 51"></p>
<p>其中$\widetilde{A}=A+I_N$是带有自连接的邻接矩阵，$\tilde{D}$就是根据$\tilde{A}$的来的度矩阵，$W^{(l)}$是各层独立的可训练的权重参数，$\sigma$是激活函数，$H^{(l)} \in R^{N \times D}$是第l层的隐藏状态矩阵,$H^{(0)}=X$。</p>
<h3 id="谱图卷积">谱图卷积</h3>
<p>谱卷积定义为信号x和过滤器$g_\theta \times =Ug_\theta U^Tx $在傅里叶域上的乘法</p>
<p><img src="https://i.imgur.com/c7vuiU7.png" alt="picture 52"></p>
<p>其中U是归一化拉普拉斯的特征向量组成的矩阵，$L=L_N-D^{- \frac{1}{2}}AD^{- \frac{1}{2}}=U\Lambda U^T$，$U^Tx$是对x的图傅里叶转换。我们可以将$g_\theta$理解成对L的特征值的函数，比如$g_\theta(\Lambda)$。但是计算公式3的计算成本太高，并且对于大的图更是如此。为此，有人提出$g_\theta (\Lambda)$可以由车比雪夫多项式的前k部分估计出来</p>
<p><img src="https://i.imgur.com/L1Monm9.png" alt="picture 53"></p>
<p>其中$\tilde{\Lambda}=\frac{2}{\lambda <em>{max}} \Lambda -I_N$，$\lambda <em>{max}$是L的最大特征值。车比雪夫多项式可以递归地定义为$T_k(x)=2xT</em>{k-1}(x)-T</em>{k-2}(x)$,其中$T_0(x)=1$,$T_1(x)=x$。</p>
<p>回到我们的定义上，现在我们有</p>
<p><img src="https://i.imgur.com/sWHyYb3.png" alt="picture 54"></p>
<p>可以看出这个表达式只关心节点的k阶邻居，这使得它的计算复杂度直线下降。</p>
<h3 id="逐层的线性模型">逐层的线性模型</h3>
<p>因此根据公式5可以堆叠多个基于图卷积的神经网络层，每一层之后更一个逐点的非线性层。现在我们对逐层的卷积操作进行限制，使它只关注一阶邻居，即K=1，这样的话就是在拉普拉斯图谱上进行一个线性操作。</p>
<p>用这种方式仍然可以通过叠加这样的层来恢复丰富的卷积滤波器函数。我们期望这种模型能够减轻在图的局部邻接关系上的过拟合问题。此外，这样的逐层线性转化使我们能够构建更深的模型。</p>
<p>我们近似将$\lambda =2$，这样公式5就可以简化为：</p>
<p><img src="https://i.imgur.com/LXQcZ2B.png" alt="picture 55"></p>
<p>实际应用中我们可以限制参数的数量来解决过拟合问题，并且最小化每一层的计算量。这样我们可以得到以下公式</p>
<p><img src="https://i.imgur.com/BNLrisG.png" alt="picture 56"></p>
<p>其中$\theta =\theta_{1}^{‘} =-\theta_{1}^{’}$。但是重复这一操作会导致数值不稳定出现梯度消失/爆炸。为了解决这一问题，这里做了个小trick，邻接矩阵使用包含自邻接关系，度矩阵也包含自邻接关系。</p>
<p>这样我们就可以将具有C个输入通道的信号矩阵$X\in R^{N\times C}$和过滤器或者特征图定义为下式</p>
<p><img src="https://i.imgur.com/jJj90Ec.png" alt="picture 57"></p>
<h2 id="自监督节点分类">自监督节点分类</h2>
<h3 id="举例">举例</h3>
<p>我们假设一个两层的GCN用于半监督节点分类。我们先在一个预处理步骤中计算出$\tilde{A}=\tilde{D}^{-\frac{1}{2}}\tilde{A}\tilde{D}^{-\frac{1}{2}}$。前向传播公式如下</p>
<p><img src="https://i.imgur.com/h5iMkZD.png" alt="picture 58"></p>
<p>其中，$W^{(0)}\in R^{C\times H}$是第一个隐藏层的参数，$W^{(1)}\in R^{H\times F}$是输出层的参数。然后我们在所有的有标签样本上计算交叉熵损失</p>
<p><img src="https://i.imgur.com/Jtsg0ac.png" alt="picture 59"></p>
<p>下图是图卷积层的示意图</p>
<p><img src="https://i.imgur.com/JGUN9Vu.png" alt="picture 60"></p>
<h2 id="文章种公式的详细理解">文章种公式的详细理解</h2>
<p>首先我们有了一张图和他的基本信息，注意这里每个节点的向量可以是通过一些embeding方式得到的，不是重点。</p>
<p><img src="https://i.imgur.com/ldUPSX0.png" alt="picture 61"></p>
<p>那我们怎么获取到周遭节点的信息来辅助更新本节点的信息呢？可以用邻接矩阵和全部节点的特征组成的矩阵进行乘法，下图可以直观地展示出为什么能够获得邻居节点的信息</p>
<p><img src="https://i.imgur.com/bySloaz.png" alt="picture 62"></p>
<p>当然，这样做还是有缺点的：</p>
<ol>
<li>缺少该节点本身的特征信息</li>
<li>如果一个节点连接的邻居特别多或者特别少，用这种sum方法就会出现梯度爆炸或者梯度消失的问题。此外神经网络对输入信息的值比较敏感，因此需要归一化</li>
</ol>
<p>对于问题一解决办法很简单，邻接矩阵包含自邻接信息就可以了。</p>
<p>对于问题二，一般做矩阵的缩放是乘以一个对角阵实现的。自然而然我们会想到使用度矩阵来实现。这里的度矩阵同样应该是包含自邻接关系的。使用度矩阵的逆，就可以实现邻接节点特征求和之后的取平均了。如下图所示</p>
<p><img src="https://i.imgur.com/ZqluyWD.png" alt="picture 63"></p>
<p>但是我们会发现，因为对角阵是在邻接矩阵左边乘的，所以实际上，只是用了$D_{ii}$对邻接矩阵的行进行了缩放，而没有对列进行。如下图所示</p>
<p><img src="https://i.imgur.com/rDiJJK0.png" alt="picture 64"></p>
<p>既然如此，我们可以尝试在右边再乘一个度矩阵的逆，这样不就行列都能得到缩放了吗？但是需要注意这里的原理是邻接矩阵是一个对称阵，所以其第i行和第i列是一样的，因此才可以这么做。</p>
<p><img src="https://i.imgur.com/qYMvgrE.png" alt="picture 65"></p>
<p>新的缩放方法给出了一个平均的权重，我们做的改进就是让他对邻接关系少的节点赋予更高的权重，这样能够减少邻接关系多的节点的影响。</p>
<p>需要注意，我们对缩放矩阵进行归一化的时候一般做两次，一次对行一次对列，这是最常用的方式。也是文中作者使用的方式。</p>
<p><img src="https://i.imgur.com/kphpELZ.png" alt="picture 66"></p>
<p>卷积层的数量意味着一个节点的特征能够传播的距离。假如我们只有一层的话，那只能获取到邻居的信息，所有节点同时聚合自己邻居的信息。但如果我们堆叠多层，一个节点的邻居就能够包含其自身邻居的信息，经过聚合后当前节点就能够获得更远的节点的信息。</p>
<p>但是一般我们不希望有太多层，经过6-7次跳跃，我们几乎已经可以聚合整张图的信息了。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://weirdozz.github.io/2022/03/19/%E8%8A%82%E7%82%B9%E8%A1%A8%E7%A4%BA%E5%A6%82%E4%BD%95%E7%94%9F%E6%88%90%E5%9B%BE%E8%A1%A8%E7%A4%BA/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/weirdo.jpg">
      <meta itemprop="name" content="Weirdo">
      <meta itemprop="description" content="怕什么真理无穷，进一步有进一步的欢喜">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Weirdo">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/03/19/%E8%8A%82%E7%82%B9%E8%A1%A8%E7%A4%BA%E5%A6%82%E4%BD%95%E7%94%9F%E6%88%90%E5%9B%BE%E8%A1%A8%E7%A4%BA/" class="post-title-link" itemprop="url">节点表示如何生成图表示</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-19 21:03:01" itemprop="dateCreated datePublished" datetime="2022-03-19T21:03:01+08:00">2022-03-19</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2022-03-24 12:35:10" itemprop="dateModified" datetime="2022-03-24T12:35:10+08:00">2022-03-24</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" itemprop="url" rel="index"><span itemprop="name">图神经网络</span></a>
        </span>
    </span>

  
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Disqus：</span>
    
    <a title="disqus" href="/2022/03/19/%E8%8A%82%E7%82%B9%E8%A1%A8%E7%A4%BA%E5%A6%82%E4%BD%95%E7%94%9F%E6%88%90%E5%9B%BE%E8%A1%A8%E7%A4%BA/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2022/03/19/节点表示如何生成图表示/" itemprop="commentCount"></span>
    </a>
  </span>
  
  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="图读出操作">图读出操作</h2>
<p>顾名思义，图读出操作就是用来生成图表示的。它的核心要义在于：操作本身要对节点顺序不敏感。</p>
<p>为什么是这样呢？因为在欧氏空间中如果一张图片旋转了，那么他就是新的图片了；但在非欧氏空间中，如果一个图旋转一下，比如重新编号，形成的图仍然是原图。这就是典型的图重构问题。下面两个图是等价的</p>
<p><img src="https://i.imgur.com/1lrMMsW.png" alt="picture 35"></p>
<p>为了使得同构图能够保持一致，图读出的操作就需要对节点顺序不敏感。数学上能表达这种操作的函数称为对称函数。</p>
<p>接下来介绍两中方法实现图读出操作：<strong>基于统计的方法</strong>和<strong>基于学习的方法</strong></p>
<h2 id="基于统计的方法">基于统计的方法</h2>
<p>有学者提出用类似直方图的方法对每维数据分布进行建模。假设我们有100个介于[-3,1]之间的数字，如果我们直接将他们求和，就看不出他们的分布情况；而如果我们将[-3,1]分成4个子区域，分开统计各个区域的和就能略微发现一些原始数据分布特征，如下图</p>
<p><img src="https://i.imgur.com/YT5Y8pE.png" alt="picture 36"></p>
<p>如果要实现上面这个做法，应该怎么做呢？很简单，举个例子，给定3个数据点，他们的特征向量（2D）分别是[-2,1],[-1,2]和[-1,1]。如果直接求和，全局的特征向量是[-2±1±1,1+2+1]即[-4,4]。如果采取上述直方图的方式，则可能会得到一个这样的全局特征向量<code>[-2, -1 + -1, 1 + 1, 2]</code>（第1,2维代表从原先的第1维统计的直方图,对应的区域为<code>[-2,1),[1,2)</code>,第3,4维的含义类似）。但在实践中并没有使用这种方法，而是采用高斯函数来实现名为模糊直方图的操作。</p>
<p><img src="https://i.imgur.com/T5HHZvP.png" alt="picture 37"></p>
<p>模糊直方图的原理也很简单：预先定义几个特征值区域的边界为各个高斯分布的均匀值，并且预设好方差。对于任一特征值，根据其与各个高斯分布交点的纵坐标作为其落入该区域的数值，然后将所有数值归一化。比如，图上的[1.8]与三个高斯分布交点分别在0，0.3，0.9处，归一化一下可以用一个三维向量<code>[0.0,0.25,0.75]</code>表示。</p>
<h2 id="基于学习的方法">基于学习的方法</h2>
<p>基于统计的方法的坏处在于它没办法参数化，间接地难以表示节点到图向量的“复杂过程”。基于学习的方法就是希望用神经网络来拟合这个过程。</p>
<h3 id="采样加全连接">采样加全连接</h3>
<p>最简单的做法，取固定数量节点，通过全连接层得到图表示。这里不论是随机采样，还是根据某些规则采样，都需要得到确定数量的节点，不够就填充。公式也很简单（$H^L指的是将采样到的节点表示拼在一起）：</p>
<p><img src="https://i.imgur.com/4wwS4yv.png" alt="picture 38"></p>
<p>这种方法很难适用于规模差距很大的图。比如训练时见过的图只有几百个节点，但测试的图可能有上千个节点，很难泛化。</p>
<h3 id="全局节点">全局节点</h3>
<p>这种做法动机很简单，考虑到图同构问题和基于统计的方法，从节点的表示生成最终图表示主要有两个难点：</p>
<ol>
<li>难以找到一个合适的根节点（图的根节点一般都是根据领域知识确定的，比如前面第一篇中讲到的化合物分类）</li>
<li>如果直接用基于统计的方法对各个节点一视同仁，无法区别对待（比如某些重要的节点信息更多，就应该表达得更多）</li>
</ol>
<p>那直接引入一个全局节点代表这张图的根节点，把他跟图中的每个节点通过一种特殊的边连接，最终拿这个节点的表示作为整个图的表示，岂不是很棒？</p>
<h3 id="可微池化">可微池化</h3>
<p>上面的两中方法都比较简单，不会层次化地去获得图表示。因此又有研究者提出了一种层次化的图的表示，而这依赖于他们提出的可微池化技术。简单来说就是，他不希望各个节点一次性得到图的表示，而是希望通过一个逐渐压缩信息的过程，来得到最终图的表示。如下图所示：</p>
<p><img src="https://i.imgur.com/QCFr9Db.png" alt="picture 39"></p>
<p>相比于一般先通过GCN获取所有节点表示，再通过方法汇总得到图的最终表示的方法，DiffPool同时完成了两个任务：<strong>节点聚类</strong>和<strong>节点表示</strong>。</p>
<p>这两个任务是由两个不共享参数的GCN模块分别完成的，下文用SC和NR分别表示这两个模块。NR模块和传统GCN一样，输入是各个节点隐藏状态，通过图上的传播，输出是传播后各个节点的i傲视。SC模块则不同，虽然输入也是各节点的隐藏表示，但输出是各节点属于不同聚类簇的概率（这里每一层聚类簇的数目是预先定义的）。上图中最左侧每个节点右上方的表格就代表这个。举个例子，假设本层子图有6个节点，将各个节点输出的簇分类概率堆叠在一起，就可以得到矩阵$S^l$，如下图所示（三个颜色代表三个聚类簇。实际中，聚类矩阵不是离散变量，而是连续变量）</p>
<p><img src="https://i.imgur.com/eDLJ1Nn.png" alt="picture 40"></p>
<p>用$A_l$表示第l层子图节点的邻接关系，$A^0$即是图的邻接矩阵，$N_l$表示第l层节点的个数，$H^l$表示第l层子图各个节点表示堆叠而成的隐状态矩阵，DiffPool通过如下公式得到新子图中各个节点的表示：</p>
<p><img src="https://i.imgur.com/B53yvW6.png" alt="picture 41"></p>
<p>除了表示各个节点的表示之外，还有一个很重要的事情是生成新子图$A^{l+1}$的邻接关系</p>
<p><img src="https://i.imgur.com/dEP1ZEr.png" alt="picture 42"></p>
<a href="/2022/03/19/%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A6%82%E8%BF%B0/" title="图神经网络概述">图神经网络概述</a>
<a href="/2022/03/19/%E5%9B%BE%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A6%82%E8%BF%B0/" title="图卷积神经网络概述">图卷积神经网络概述</a>
<a href="/2022/03/19/%E8%8A%82%E7%82%B9%E8%A1%A8%E7%A4%BA%E5%A6%82%E4%BD%95%E7%94%9F%E6%88%90%E5%9B%BE%E8%A1%A8%E7%A4%BA/" title="节点表示如何生成图表示">节点表示如何生成图表示</a>
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://weirdozz.github.io/2022/03/19/%E5%9B%BE%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A6%82%E8%BF%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/weirdo.jpg">
      <meta itemprop="name" content="Weirdo">
      <meta itemprop="description" content="怕什么真理无穷，进一步有进一步的欢喜">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Weirdo">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/03/19/%E5%9B%BE%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A6%82%E8%BF%B0/" class="post-title-link" itemprop="url">图卷积神经网络概述</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-19 07:37:08" itemprop="dateCreated datePublished" datetime="2022-03-19T07:37:08+08:00">2022-03-19</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2022-03-24 12:35:10" itemprop="dateModified" datetime="2022-03-24T12:35:10+08:00">2022-03-24</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" itemprop="url" rel="index"><span itemprop="name">图神经网络</span></a>
        </span>
    </span>

  
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Disqus：</span>
    
    <a title="disqus" href="/2022/03/19/%E5%9B%BE%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A6%82%E8%BF%B0/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2022/03/19/图卷积神经网络概述/" itemprop="commentCount"></span>
    </a>
  </span>
  
  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="图卷积缘起">图卷积缘起</h2>
<p>我们要思考：为什么研究者们要设计图卷积，直接在图上不能应用传统的卷积吗？要理解这个，我们首先要理解应用传统卷积的图像（欧氏空间）和图（非欧式空间的区别）。如下图，是在两种不同结构上应用卷积的场景</p>
<p><img src="https://i.imgur.com/PrBt36X.png" alt="picture 6"></p>
<p>图卷积真正的难点在于 <strong>邻居节点数量不固定</strong>。目前有两中主流的研究方向</p>
<ul>
<li>将非欧式空间转换为欧式空间</li>
<li>找出一种可以处理变长邻居节点的卷积核在图上抽取特征</li>
</ul>
<h2 id="图卷积的框架">图卷积的框架</h2>
<p>如下图所示，输入的是整张图，在<code>Convolution Layer 1</code>里面，对每个节点的邻居都进行一次卷积操作，并且用卷积的结果更新这个节点；然后经过激活函数传给<code>Convolution Layer 2</code>与激活函数，重复这个过程。</p>
<p>与GNN类似，GCN也有一个局部输出函数，用于将节点的状态（包括隐藏状态和节点特征）转换成任务相关的标签，称为<code>Node-Level</code>的任务；也有些任务是对整张图进行分类的，比如化合物分类，称为<code>Graph-Level</code>的任务。卷积操作只关心每个节点的隐藏状态如何更新，而对于<code>Graph-Level</code>的任务，会在卷积层之后增加更多的操作。</p>
<h2 id="卷积">卷积</h2>
<p>图卷积神经网络主要两中：空域和频域的。通俗点，空域就是直接在图片的像素点上进行卷积，频域是对图片进行傅里叶变换后再卷积。</p>
<h3 id="空域卷积">空域卷积</h3>
<p>其核心在于聚合邻居节点的信息。比如最简单的无参卷积方式是：将所有直连邻居节点的隐藏状态加和，来更新当前节点的隐藏状态。</p>
<p><img src="https://i.imgur.com/3Jnfovx.png" alt="picture 14"></p>
<h3 id="消息传递网络（Message-Passing-Neural-Network）">消息传递网络（Message Passing Neural Network）</h3>
<p>严格来说，MPNN并不是一种具体的模型，而是一种空域卷积的形式化框架。将空域卷积分解为两个过程：消息传递和状态更新，分别由$M_l(·)$和$U_l(·)$函数完成。将节点v的特征$X_v$作为隐藏状态的初始状态$H_v^0$之后，空域卷积对隐藏状态的更新如下：</p>
<p><img src="https://i.imgur.com/dmxgXS1.png" alt="picture 15"></p>
<p>其中l代表卷积层的第l层。式子的物理意义是：受到来自每个邻居的消息$M_{l+1}$之后，每个节点如何更新自己的状态。</p>
<p>MPNN的示意图如下</p>
<p><img src="https://i.imgur.com/aaerMwm.png" alt="picture 16"></p>
<h3 id="图采样与聚合">图采样与聚合</h3>
<p>MPNN很好地概括了空域卷积的过程，但这个框架下的所有模型都有一个共同的缺陷：卷积操作针对的对象是整张图，即要将所有的节点放到内存中，才能卷积。但是实际应用中，整个图上的卷积操作并不现实。GraphSage利用采样部分节点的方式进行学习。当然，即使不需要整张图同时卷积，GraphSage仍然需要聚合邻居节点的信息。这种操作类似于MPNN中的消息传递过程</p>
<p><img src="https://i.imgur.com/N3phwJo.png" alt="picture 17"></p>
<p>具体采样过程分为三步：</p>
<ol>
<li>在图中随机采样若干个节点，节点数就是传统任务中的<code>batch_size</code>。对于每个节点，随机选择固定数目的邻居节点（未必是一阶邻居，也可以是二阶邻居）。</li>
<li>将邻居节点的信息通过aggregate函数聚合起来更新刚才采样的节点</li>
<li>计算采样节点处的损失。如果是无监督任务，我们希望图上邻居节点的编码相似；如果是监督任务，可以根据具体节点的任务标签计算损失。</li>
</ol>
<p>最终的GraphSage的状态更新公式如下：</p>
<p><img src="https://i.imgur.com/e6fe9aR.png" alt="picture 43"></p>
<p>GraphSage的设计重点是aggrega函数的设计上，它可以是max，mean也可以是带参数的如LSTM等神经网络。</p>
<h2 id="图结构序列化">图结构序列化</h2>
<p>这个方法将图结构转换成序列结构，然后直接利用卷积神经网络在序列化之后的序列结构上进行卷积。</p>
<p>这种序列转换需要保持图结构的两个特点：1.同构的性质。2.邻居节点的链接关系。对于前者，意思是我们把图上节点的标号随机打乱，得到的仍然应该是一样的序列；对于后者，意味着我们需要保持邻居节点和目标节点的距离关系。</p>
<p>PATCH-SAN通过以下三个步骤来解决这两个问题：</p>
<ol>
<li><strong>节点选择</strong>：通过一些人为定义的规则（比如度大的节点分数高，邻居节点度大时分数较高等）为每个节点指定一个在图中的排序。完成排序后取出前$\omega$个节点代表整个图。</li>
<li><strong>邻居节点构造</strong>：以第一步选择的节点为中心，得到他们的邻居节点（可以是一阶邻居也可以是二阶邻居），这样就构成了$\omega$个团，根据第一步得到的节点排序对每个团中的邻居节点进行排序，取前k个邻居节点按照顺序排序，就组成了$\omega$个有序的团。</li>
<li><strong>图规范化</strong>：按照每个团中的节点顺序可以将所有的团转换成固定长度的序列（k+1），再将他们按照中心节点的顺序从前到后依次拼接就能得到长度为$\omega \times (k+1)$的代表整张图的序列。需要注意的是，如果取不到$\omega$或者k个节点，需要用空节点填充。</li>
</ol>
<p>流程图如下所示：</p>
<p><img src="https://i.imgur.com/qEhL4bU.png" alt="picture 24"></p>
<p>下图更清晰：</p>
<p><img src="https://i.imgur.com/aRKWzoN.png" alt="picture 25"></p>
<h2 id="频域卷积">频域卷积</h2>
<p>空域卷积直观地借鉴了图像中的卷积操作，但是缺乏理论基础。而频域卷积则不同，主要利用图傅里叶变换实现卷积。</p>
<p>简单来说就是，利用图的拉普拉斯矩阵导出其频域上的拉普拉斯算子，再类比频域上的欧氏空间中的卷积，导出卷积公式。</p>
<h3 id="傅里叶变换">傅里叶变换</h3>
<p>傅里叶变换会将一个在空域（或者时域）上定义的函数分解成频域上的若干频率成分。即傅里叶变换可以将一个函数从空域变换到频域。先抛开傅里叶变换的数学公式，用F来表示傅里叶变换的话，有一个重要的恒等式：</p>
<p>$$<br>
(f * g)(t)=F^{-1} [F [f(t)] \odot F [g(t)] ]<br>
$$</p>
<p>这里的$F^{-1}$ 指的是傅里叶逆变换，$\odot$ 指的是哈达玛积。上式的直观含义是：空（时）域卷积等于频域卷积。简单来说就是如果要求f和g的卷积，可以先将他们通过傅里叶变换变换到频域中，将两个函数在频域中相乘，然后再通过傅里叶逆变换转换出来。</p>
<p>那么傅里叶变换能够做什么呢？一个简单的应用是给图像去除规律噪点。如下图，傅里叶变换前有一些规律的条纹，直接在原图去除有点困难，但我们可以将图片通过傅里叶变换变到频谱图中，频谱中的规律条纹就是原图中的背景条纹。</p>
<p><img src="https://i.imgur.com/1RCTpbS.png" alt="picture 26"></p>
<p>只要在频谱图中去除这些点，就可以将背景条纹去掉。</p>
<p><img src="https://i.imgur.com/ocm1zaI.png" alt="picture 27"></p>
<p>除了去除噪声点，其在加速卷积运算方面还有很大的潜力，快速傅里叶变换也是由此而生。事实上现在常用的卷积神经网络完全可以搭配傅里叶变换。但是由于目前的大部分卷积核都比较小，卷积操作本身就不是很大开销，搭配傅里叶变换不一定就会减小时间开销。</p>
<p>那么傅里叶变换的公式是什么样的呢？</p>
<p>$$<br>
\hat{f}(t)=\int f(x) \exp ^{-2 \pi i x t} d x<br>
$$</p>
<p>其中$i=\sqrt{-1}$，t是任意实数。</p>
<p>我们这里实际上需要关注$\exp ^{-2 \pi i x t}$的物理意义，它实际上是拉普拉斯算子$\delta$的广义特征函数。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">拉普拉斯算子(Laplacian operator) 的物理意义是空间二阶导，准确定义是：标量梯度场中的散度，一般可用于描述物理量的流入流出。比如说在二维空间中的温度传播规律，一般可以用拉普拉斯算子来描述。</span><br></pre></td></tr></table></figure>
<p>为什么是特征向量呢？稍微推导以下就可以知道，拉普拉斯算子作用在$\exp ^{-2 \pi i x t}$确实满足以上特征向量的定义：</p>
<p>$$<br>
\Delta \exp ^{-2 \pi i x t}=\frac{\partial^{2} } {\partial t^{2} } \exp ^{-2 \pi i x t}=-4 \pi^{2} x^{2} \exp ^{-2 \pi i x t}<br>
$$</p>
<p>这里$\partial$是求导符号，$\partial^2$是二阶导。</p>
<p>实际上，仔细观察傅里叶变换的例子，本质上就是将函数f(t)映射到$\exp ^{-2\pi ixt}$的基向量空间中。</p>
<h3 id="图上的傅里叶变换">图上的傅里叶变换</h3>
<p>前面讲的都是为了这一部分做铺垫。问题来了：在图上，我们怎么找拉普拉斯算子$\delta$和$\exp ^{-2\pi ixt}$呢？</p>
<p>研究者们找到了图的拉普拉斯矩阵和特征向量作为替代品。拉普拉斯矩阵本质上是度矩阵（D）减去邻接矩阵（A）L=D-A，如图所示</p>
<p><img src="https://i.imgur.com/xkIhW3R.png" alt="picture 28"></p>
<p>频域卷积的前提条件就是图必须是无向图，那么L就是对称矩阵。所以可以由以下公式分解：</p>
<p><img src="https://i.imgur.com/zJw8FYb.png" alt="picture 29"></p>
<p>那么根据上面卷积和傅里叶结合的变换公式，图上频域卷积的公式就可以写成$\hat{f}(t)=\sum_{n=1}^{N} f(n) u_{t}(n)$，如果整个图上的N个节点一起做卷积，就能够得到整张图上的卷积如下：</p>
<p><img src="https://i.imgur.com/oZNB2qd.png" alt="picture 30"></p>
<p>回顾传统卷积和图卷积，发现他们非常相似，这里f都是特征函数，g都是卷积核</p>
<p><img src="https://i.imgur.com/hxBFtSB.png" alt="picture 31"></p>
<p>如果把$U^Tg$整体看作可以学习的卷积核，我们写作$g_\theta$。最终的卷积公式就是：</p>
<p><img src="https://i.imgur.com/6ZqiOgV.png" alt="picture 32"></p>
<h3 id="频域卷积网络">频域卷积网络</h3>
<p>上面推导出的$g_\theta$就是首个提出的频域卷积神经网络的卷积核。设l层的隐藏状态为$h^l\in R^{N\times d_t}$，频域卷积层的状态更新计算公式如下：</p>
<p><img src="https://i.imgur.com/K0CkMxN.png" alt="picture 33"></p>
<p>物理上可以如下理解</p>
<p><img src="https://i.imgur.com/lldUmjS.png" alt="picture 34"></p>
<a href="/2022/03/19/%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A6%82%E8%BF%B0/" title="图神经网络概述">图神经网络概述</a>
<a href="/2022/03/19/%E5%9B%BE%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A6%82%E8%BF%B0/" title="图卷积神经网络概述">图卷积神经网络概述</a>
<a href="/2022/03/19/%E8%8A%82%E7%82%B9%E8%A1%A8%E7%A4%BA%E5%A6%82%E4%BD%95%E7%94%9F%E6%88%90%E5%9B%BE%E8%A1%A8%E7%A4%BA/" title="节点表示如何生成图表示">节点表示如何生成图表示</a>
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://weirdozz.github.io/2022/03/19/%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A6%82%E8%BF%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/weirdo.jpg">
      <meta itemprop="name" content="Weirdo">
      <meta itemprop="description" content="怕什么真理无穷，进一步有进一步的欢喜">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Weirdo">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/03/19/%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A6%82%E8%BF%B0/" class="post-title-link" itemprop="url">图神经网络概述</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-19 05:12:47" itemprop="dateCreated datePublished" datetime="2022-03-19T05:12:47+08:00">2022-03-19</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2022-03-24 12:35:10" itemprop="dateModified" datetime="2022-03-24T12:35:10+08:00">2022-03-24</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" itemprop="url" rel="index"><span itemprop="name">图神经网络</span></a>
        </span>
    </span>

  
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Disqus：</span>
    
    <a title="disqus" href="/2022/03/19/%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A6%82%E8%BF%B0/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2022/03/19/图神经网络概述/" itemprop="commentCount"></span>
    </a>
  </span>
  
  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="状态更新与输出">状态更新与输出</h2>
<p>最早的图神经网络的理论基础是不动点理论。给定一张图$G$，每个节点都有其自身特征$X_v$，每个边也有其自身特征$X_{(u,v)}$。GNN的学习目标就是获得每个结点的图感知的隐藏状态，即：对于每个节点，其隐藏状态包含来自邻居节点的信息。那如何让每个节点都能感知到图上的其他节点呢？GNN通过迭代式更新所有节点的隐藏状态来实现，在t+1时刻，节点的隐藏状态按照如下方式更新：</p>
<p><img src="https://i.imgur.com/2vgW7gB.png" alt="picture 8"></p>
<p>其中f就是隐藏状态的更新函数，$x_co[v]$是与节点v相邻的边的特征，$x_ne[v]$是节点v的邻居节点的特征，$h_n^te[v]$是邻居节点在t时刻的隐藏状态。神经网络需要做的就是拟合这个函数f。</p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2022/03/19/%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A6%82%E8%BF%B0/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://weirdozz.github.io/2022/03/17/github%E6%97%A0%E6%B3%95%E7%94%A8%E5%91%BD%E4%BB%A4%E8%A1%8C%E8%BF%9E%E6%8E%A5/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/weirdo.jpg">
      <meta itemprop="name" content="Weirdo">
      <meta itemprop="description" content="怕什么真理无穷，进一步有进一步的欢喜">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Weirdo">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/03/17/github%E6%97%A0%E6%B3%95%E7%94%A8%E5%91%BD%E4%BB%A4%E8%A1%8C%E8%BF%9E%E6%8E%A5/" class="post-title-link" itemprop="url">github无法用命令行连接</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-17 16:11:40" itemprop="dateCreated datePublished" datetime="2022-03-17T16:11:40+08:00">2022-03-17</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2022-03-24 12:35:10" itemprop="dateModified" datetime="2022-03-24T12:35:10+08:00">2022-03-24</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/github/" itemprop="url" rel="index"><span itemprop="name">github</span></a>
        </span>
    </span>

  
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Disqus：</span>
    
    <a title="disqus" href="/2022/03/17/github%E6%97%A0%E6%B3%95%E7%94%A8%E5%91%BD%E4%BB%A4%E8%A1%8C%E8%BF%9E%E6%8E%A5/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2022/03/17/github无法用命令行连接/" itemprop="commentCount"></span>
    </a>
  </span>
  
  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>如果在使用git命令的时候出现<code>ssh: connect to host github.com port 22: Connection timed out</code>,可能是host文件配置错误，我们打开host文件（win10一般在C:\Windows\System32\drivers\etc）。在最下方添加github ip。</p>
<ul>
<li>查看github的ip地址，进入网址：<a target="_blank" rel="noopener" href="https://github.com.ipaddress.com">https://github.com.ipaddress.com</a></li>
<li>确定域名ip，进入网址：<a target="_blank" rel="noopener" href="https://fastly.net.ipaddress.com/github.global.ssl.fastly.net">https://fastly.net.ipaddress.com/github.global.ssl.fastly.net</a></li>
<li>确定静态资源ip（应该是有4个），进入网址：<a target="_blank" rel="noopener" href="https://github.com.ipaddress.com/assets-cdn.github.com">https://github.com.ipaddress.com/assets-cdn.github.com</a></li>
</ul>
<p>得到上述ip后添加到记事本最后，格式如下</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">140.82.113.4 github.com</span><br><span class="line">199.232.69.194 github.global.ssl.fastly.net</span><br><span class="line">185.199.108.153 assets-cdn.github.com</span><br><span class="line">185.199.109.153 assets-cdn.github.com</span><br><span class="line">185.199.110.153 assets-cdn.github.com</span><br><span class="line">185.199.111.153 assets-cdn.github.com</span><br></pre></td></tr></table></figure>
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://weirdozz.github.io/2022/03/16/EvolveGCN-Evolving-Graph-Convolutional-Networks-for-Dynamic-Graphs/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/weirdo.jpg">
      <meta itemprop="name" content="Weirdo">
      <meta itemprop="description" content="怕什么真理无穷，进一步有进一步的欢喜">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Weirdo">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/03/16/EvolveGCN-Evolving-Graph-Convolutional-Networks-for-Dynamic-Graphs/" class="post-title-link" itemprop="url">EvolveGCN:Evolving Graph Convolutional Networks for Dynamic Graphs</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-16 21:17:17" itemprop="dateCreated datePublished" datetime="2022-03-16T21:17:17+08:00">2022-03-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2022-03-24 12:35:10" itemprop="dateModified" datetime="2022-03-24T12:35:10+08:00">2022-03-24</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B/" itemprop="url" rel="index"><span itemprop="name">异常检测</span></a>
        </span>
    </span>

  
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Disqus：</span>
    
    <a title="disqus" href="/2022/03/16/EvolveGCN-Evolving-Graph-Convolutional-Networks-for-Dynamic-Graphs/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2022/03/16/EvolveGCN-Evolving-Graph-Convolutional-Networks-for-Dynamic-Graphs/" itemprop="commentCount"></span>
    </a>
  </span>
  
  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="摘要">摘要</h2>
<p>归功于对于欧几里得数据（排列整齐，能够很容易找到邻居节点，就在旁边不偏不倚，如图片、视频、语音等）的深度学习的广泛使用，在非欧几里得领域出现了许多有创造性的神经网络，作为其中的代表，图表示学习慢慢回到主流研究方向。随着图神经网络在静态场景下应用的成功，作者更进一步接近实际应用场景，其中的图是动态变化的。现存的方法主要是应用节点嵌入并且用RNN来规范嵌入从而学习时间动态性。这些方法需要知道节点在整个时间跨度上的信息而且不能用于频繁变化的节点集。在一些极端场景下，节点集在不同的时间步上可能是完全不同的。</p>
<p>为了解决这个问题，作者提出EvolveGCN，跟着时间维度调节图卷积网络而不需要使用节点嵌入。该方法通过RNN来获取图序列的动态从而调整GCN的参数。参数的调整考虑到了两种结构。</p>
<p>作者在连接预测、边分类和节点分类三个任务上评估了提出的方法，效果相当好。<a target="_blank" rel="noopener" href="https://github.com/IBM/EvolveGCN">github开源代码链接</a></p>
<h2 id="引言">引言</h2>
<p>图是无处不在的数据结构，用于对实体间的成对交互进行建模。相较于欧几里得数据，通过图进行学习会遭遇很多独特的问题，包括他们的组合性质和可扩展性瓶颈。</p>
<p>目前使用图学习的神经网络主要集中于给定的静态图。而在现实应用中，图结构通常是动态的（比如一个人的社交网络是会随着时间不断改变的）。这种情况下需要更新节点嵌入来获取其变化。</p>
<p>作者基于用于静态图的图神经网络，通过引入循环机制来更新网络参数从而应用于动态场景。<strong>大量的 GNN 通过递归地聚合来自单跳邻域的节点嵌入来执行信息融合</strong>。网络的大部分参数是每一层中节点嵌入的线性变换。</p>
<p>许多相近的方法都是使用GNN来提取特征然后使用RNN来学习提取到的特征（节点嵌入）中的序列信息。最后为时间轴上所有的图学习到一个单一的GNN模型。这些模型的缺点在于他们需要获取节点在整个时间跨度上的信息并且难以保证在未来某个新节点上的表现。</p>
<p>实际应用中，除了会在训练之后产生新的节点之外，节点本身也可能频繁地出现或者小时，这使得节点嵌入的方法是有问题的因为RNN难以学习到这种无规律的行为。</p>
<p>为了解决这些问题，作者提出了在每一个时间步上使用RNN来调节GCN模型的参数。这个方法能够高效地执行模型调整，集中于模型本身而非节点嵌入。因此，不限制节点位置的改变。此外，对于未来图中会出现的新节点EvolceGCN仍然很敏感。</p>
<p>注意，GCN的参数是不训练的，他们是根据RNN得来的，因此只有RNN的参数是训练得来的。通过这种方式，参数的数量不会随着时间增加而增加而且模型就像一个经典RNN一样可控。</p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2022/03/16/EvolveGCN-Evolving-Graph-Convolutional-Networks-for-Dynamic-Graphs/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://weirdozz.github.io/2022/03/09/go-web-%E7%BC%96%E7%A8%8B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/weirdo.jpg">
      <meta itemprop="name" content="Weirdo">
      <meta itemprop="description" content="怕什么真理无穷，进一步有进一步的欢喜">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Weirdo">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/03/09/go-web-%E7%BC%96%E7%A8%8B/" class="post-title-link" itemprop="url">go web 编程</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-09 15:21:53" itemprop="dateCreated datePublished" datetime="2022-03-09T15:21:53+08:00">2022-03-09</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2022-03-24 13:45:01" itemprop="dateModified" datetime="2022-03-24T13:45:01+08:00">2022-03-24</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/go/" itemprop="url" rel="index"><span itemprop="name">go</span></a>
        </span>
    </span>

  
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Disqus：</span>
    
    <a title="disqus" href="/2022/03/09/go-web-%E7%BC%96%E7%A8%8B/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2022/03/09/go-web-编程/" itemprop="commentCount"></span>
    </a>
  </span>
  
  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="web基础">web基础</h2>
<h3 id="web工作方式">web工作方式</h3>
<h4 id="http请求包">http请求包</h4>
<p>Request 包分为 3 部分，第一部分叫 Request line（请求行）, 第二部分叫 Request header（请求头）, 第三部分是 body（主体）。header 和 body 之间有个空行，请求包的例子所示:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">GET /domains/example/ HTTP/1.1      // 请求行: 请求方法 请求 URI HTTP 协议/协议版本</span><br><span class="line">Host：www.iana.org               // 服务端的主机名</span><br><span class="line">User-Agent：Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.4 (KHTML, like Gecko) Chrome/22.0.1229.94 Safari/537.4          // 浏览器信息</span><br><span class="line">Accept：text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8  // 客户端能接收的 mine</span><br><span class="line">Accept-Encoding：gzip,deflate,sdch       // 是否支持流压缩</span><br><span class="line">Accept-Charset：UTF-8,*;q=0.5        // 客户端字符编码集</span><br><span class="line">// 空行,用于分割请求头和消息体</span><br><span class="line">// 消息体,请求资源参数,例如 POST 传递的参数</span><br></pre></td></tr></table></figure>
<p><img src="https://i.imgur.com/uTPJIPq.png" alt="picture 1"></p>
<p>fiddler抓取的GET信息</p>
<p><img src="https://i.imgur.com/1ZOWDms.png" alt="picture 3"></p>
<p>fiddler抓取的POST信息</p>
<p>可以看出GET请求体为空，POST请求体存在内容。</p>
<ul>
<li>GET提交的数据放在URL后用<code>?</code>分割URL和传输数据，不同参数间用<code>&amp;</code>相连。POST方法是将提交的数据放在HTTP包的body中</li>
<li>GET提交的数据有限（因为URL的长度有限），POST方法无限</li>
<li>GET提交方法存在安全问题，比如账户密码用GET提交的话会显式地展示在URL上</li>
</ul>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2022/03/09/go-web-%E7%BC%96%E7%A8%8B/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Weirdo</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  




  <script src="/js/third-party/pace.js"></script>

  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js","integrity":"sha256-r+3itOMtGGjap0x+10hu6jW/gZCzxHsoKrOd7gyRSGY="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


<script class="next-config" data-name="disqus" type="application/json">{"enable":true,"shortname":"weirdoblog","count":true,"i18n":{"disqus":"disqus"}}</script>
<script src="/js/third-party/comments/disqus.js"></script>

</body>
</html>
